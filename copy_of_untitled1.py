# -*- coding: utf-8 -*-
"""Copy of Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aIR1fhs1xzo7hlpBgk_MgK81URMoDgYE
"""

!pip install playwright pandas nest_asyncio

!playwright install chromium
!playwright install-deps

import nest_asyncio
nest_asyncio.apply()

import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import re
import random

# --- CONFIGURATION DATA ---
URL = "https://www.nike.com/ph/w"
DEBUG_LIMIT = 50  # Set to None to scrape EVERYTHING (taking longer)

async def run_scraper():
    async with async_playwright() as p:

        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()

        print(f" Navigating to {URL}...")
        await page.goto(URL, timeout=60000)

        # --- PART 1: INFINITE SCROLL (Get All Grid Items) ---
        print(" Scrolling to load products...")
        last_height = await page.evaluate("document.body.scrollHeight")

        while True:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(2) # Wait for load
            new_height = await page.evaluate("document.body.scrollHeight")

            # Check if we hit the bottom or the debug limit
            if new_height == last_height:
                break
            last_height = new_height

            # Stop early if testing
            count = await page.locator('.product-card').count()
            if DEBUG_LIMIT and count >= DEBUG_LIMIT:
                break

        print(" Scrolling complete.")

        # --- PART 2: EXTRACT GRID ---
        print(" Extracting product data...")
        product_elements = await page.locator('.product-card').all()
        products = []

        for product_element in product_elements:
            name_element = await product_element.locator('.product-card__title').first.all_text_contents()
            name = name_element[0].strip() if name_element else "N/A"

            link_element = await product_element.locator('a.product-card__link-overlay').first.get_attribute('href')
            link = link_element.strip() if link_element else "N/A"

            price_element = await product_element.locator('.product-price').first.all_text_contents()
            price = price_element[0].strip() if price_element else "N/A"

            image_element = await product_element.locator('img.product-card__hero-image').first.get_attribute('src')
            image_url = image_element.strip() if image_element else "N/A"

            # Tagging Rule: Gender
            gender_tag = "N/A"
            if re.search(r'men', name, re.IGNORECASE):
                gender_tag = "Men"
            elif re.search(r'women', name, re.IGNORECASE):
                gender_tag = "Women"
            elif re.search(r'unisex', name, re.IGNORECASE):
                gender_tag = "Unisex"
            elif re.search(r'boys|girls|kids|youth', name, re.IGNORECASE):
                gender_tag = "Kids"
            elif re.search(r'baby|infant', name, re.IGNORECASE):
                gender_tag = "Infant"
            elif re.search(r'toddler', name, re.IGNORECASE):
                gender_tag = "Toddler"

            # Tagging Rule: Category
            category_tag = "N/A"
            if re.search(r'shoe|sneaker|runner|trainer', name, re.IGNORECASE):
                category_tag = "Footwear"
            elif re.search(r't-shirt|shirt|top|jersey|hoodie|jacket|sweatshirt|bra|vest|tank', name, re.IGNORECASE):
                category_tag = "Apparel"
            elif re.search(r'short|pant|legging|tight|trouser|jogger', name, re.IGNORECASE):
                category_tag = "Bottoms"
            elif re.search(r'bag|backpack|cap|hat|sock|glove|ball|equipment', name, re.IGNORECASE):
                category_tag = "Accessories"

            products.append({
                'Name': name,
                'Link': link,
                'Price': price,
                'Image URL': image_url,
                'Gender Tag': gender_tag,
                'Category Tag': category_tag
            })

        print(f" Extracted {len(products)} products.")

        # --- PART 3: CREATE DATAFRAME AND SAVE TO CSV ---
        print(" Creating DataFrame and saving to CSV...")
        df = pd.DataFrame(products)
        file_name = "nike_products.csv"
        df.to_csv(file_name, index=False)
        print(f" Data saved to {file_name}")

        await browser.close()
        print(" Scraper finished.")

await run_scraper()

from google.colab import files

try:
    files.download('nike_products.csv')
except Exception as e:
    print("Error downloading file:", e)

from google.colab import drive
drive.mount('/content/drive')